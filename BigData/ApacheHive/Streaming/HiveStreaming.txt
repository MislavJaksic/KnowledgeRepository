## Hive Streaming

Allows data to be appended continuously to a Hive table.

### Hive configuration

Before Hive 3:
https://cwiki.apache.org/confluence/display/Hive/Streaming+Data+Ingest
https://community.hortonworks.com/articles/49949/test-7.html
https://github.com/mfjohnson/HiveStreamingExamples/tree/master/src/main/java
Hive 3:
https://cwiki.apache.org/confluence/display/Hive/Streaming+Data+Ingest+V2

In the "hive-site.xml" set the following configuration properties:
´´´
hive.txn.manager                 = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager
hive.compactor.initiator.on      = true
hive.compactor.worker.threads    = 1 (greater then 0)

hive.support.concurrency         = true
hive.enforce.bucketing           = true
hive.exec.dynamic.partition.mode = nonstrict
hive.txn.timeout                 = 300
´´´

### Additional considerations

A streaming table has to be configured as follows:
* has to be stored as ORC (syntax: "stored as orc")
* has to be transactional (syntax: "tblproperties("transactional"="true")")
* must be bucketed but not sorted (syntax: "clustered by (_column_name) into 10 buckets")
* the client user has to have permissions to write to the table and create partitions (set permission???)

Example:
´´´
CREATE TABLE alarms (stamp TIMESTAMP, state STRING, status_code STRING, down TINYINT, type STRING, info STRING)
CLUSTERED BY(status_code) INTO 4 BUCKETS
STORED AS ORC
tblproperties("transactional"="true");
´´´

Transactions have a few strage behaviours. Be careful.

### Maven dependencies

Double check the version of Maven dependencies. You can check which version you need by inspecting which
.jar files Hive uses.
´´´
<dependency>
    <groupId>org.apache.hive.hcatalog</groupId>
    <artifactId>hive-hcatalog-streaming</artifactId>
    <version>HIVE_VERSION</version>
</dependency>
<dependency>
    <groupId>org.apache.hive.hcatalog</groupId>
    <artifactId>hive-hcatalog-core</artifactId>
    <version>HIVE_VERSION</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-hdfs</artifactId>
    <version>HADOOP_VERSION</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-common</artifactId>
    <version>HADOOP_VERSION</version>
</dependency>
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-mapreduce-client-core</artifactId>
    <version>HADOOP_VERSION</version>
</dependency>
<dependency> <!-- Optional -->
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-jdbc-handler</artifactId>
    <version>JDBC_HANDLER</version>
</dependency>
´´´

If a class is still missing, you can use the jar-explorer utility to find where the missing class is located.
