## Hive

Apache Hive is a data warehouse built on top of Apache Hadoop.


### Installation

https://cwiki.apache.org/confluence/display/Hive/AdminManual+Installation

Download Hive. Unpack with "tar -xzvf hive-x.y.z.tar.gz".

Add to .bashrc:
´´´
export HIVE_HOME=/path/to/your/hive
export PATH=$HIVE_HOME/bin:$PATH
´´´
In addition, add to .bashrc:
´´´
HADOOP_HOME=/path/to/your/hadoop
´´´

Run Hadoop deamons, both start-dfs.sh and start-yarn.sh.
Execute the following commands:
´´´
$HADOOP_HOME/bin/hadoop fs -mkdir       /tmp
$HADOOP_HOME/bin/hadoop fs -mkdir       /user/hive/warehouse
$HADOOP_HOME/bin/hadoop fs -chmod g+w   /tmp
$HADOOP_HOME/bin/hadoop fs -chmod g+w   /user/hive/warehouse
´´´
You can always check the HDFS with "$HADOOP_HOME/bin/hadoop fs -ls /".
If you cannot create a folder it means that you just have to construct the previous one first.

Try running "$HIVE_HOME/bin/hive".
If you get an error, see below.
´´´
<!-- Solution to the "... Relative path in absolute URI" problem -->
  <property>
    <name>system:java.io.tmpdir</name>
    <value>/tmp/hive/java</value>
  </property>
  <property>
    <name>system:user.name</name>
    <value>${user.name}</value>
  </property>
  <!-- ... solution ends -->
´´´

Instead of using HiveCLI which is deprecated, you can use Beeline and HiveServer2.

https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2

The following is already configured:
´´´
hive.server2.thrift.min.worker.threads – Minimum number of worker threads, default 5.
hive.server2.thrift.max.worker.threads – Maximum number of worker threads, default 500.
hive.server2.thrift.port – TCP port number to listen on, default 10000.
hive.server2.thrift.bind.host – TCP interface to bind to, NOT SET.

hive.server2.transport.mode - Set to http to enable HTTP transport mode, default binary.
hive.server2.thrift.http.port - HTTP port number to listen on, default 10001.
hive.server2.thrift.http.max.worker.threads - Maximum worker threads in the server pool, default 500.
hive.server2.thrift.http.min.worker.threads - Minimum worker threads in the server pool, default 5.
hive.server2.thrift.http.path - The service endpoint, default cliservice.
´´´

The server can run in either HTTP or TCP mode, not both.
Start HiveServer2 using "bin/hiveserver2". If everything starts correctly, there will be no prompts.

Start Beeline with "bin/beeline".
Execute "!connect jdbc:hive2://localhost:10000". If connecting remotely, use "bin/beeline -u jdbc:hive2://$HS2_HOST:$HS2_PORT".
Test the connection with "!tables". If it works, you are in!


https://cwiki.apache.org/confluence/display/Hive/HiveDerbyServerMode

Install and configure Apache Derby.
TODO... Installing Derby.


https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration
TODO... Fine tune the config.


### (Unofficial) installation

https://www.tutorialspoint.com/hive/hive_installation.htm

You need to have both Java and Hadoop installed.

Download Hive. Unpack it with "tar -xzvf apache-hive-x.y.z-bin.tar.gz".

Add to .bashrc.
´´´
export HIVE_HOME=/path/to/your/hive !!
export PATH=$PATH:$HIVE_HOME/bin
export CLASSPATH=$CLASSPATH:/path/to/your/hadoop/lib/*:.
export CLASSPATH=$CLASSPATH:/path/to/your/hive/lib/*:.
´´´

source ~/.bashrc - refresh the file

Check paths and create a config file with:
cd $HIVE_HOME/conf
cp hive-env.sh.template hive-env.sh


In the following files append the following:

* hive.env.sh
´´´
export HADOOP_HOME=/path/to/your/hadoop
´´´

TODO Stopped at Step 8: Verifying Hive Installation ...


### Test installation

Don't forget to run start-dfs.sh, also known as Hadoop, first.

Test using bin/hive in the /path/to/your/hive folder.


### Hive partitions

You should think of partitions as folders. These values are not part of any row, but they can be used for analasys.

Example:
´´´
CREATE TABLE alarms (stamp TIMESTAMP, state STRING, status_code STRING, is_up TINYINT, alarm_type STRING, info STRING)
PARTITIONED BY (data_name STRING, year STRING, month STRING, ip STRING)
CLUSTERED BY(status_code) INTO 4 BUCKETS
STORED AS ORC;
´´´

With Hive 3.0.0 you can dynamically partition data while streaming. (Source: https://cwiki.apache.org/confluence/display/Hive/Streaming+Data+Ingest+V2#HiveStreamingConnection)
You can also partition data dynamically by selecting and then inserting it into a partitioned table. (Source: https://cwiki.apache.org/confluence/display/Hive/DynamicPartitions)

Example:
´´´
INSERT OVERWRITE TABLE alarms PARTITION (data_name = "alarms_raw", year, month, ip)
SELECT stamp, state, status_code, is_up, alarm_type, info, year, month, ip FROM unpartitioned_alarms;
´´´
Note: data_name is partitioned statically; year, month and ip are partitioned dynamically.


### Hive test

After starting Hadoop with "start-dfs.sh" and "start-yarn.sh" and HiveServer2 with "bin/hiveserver2" do the
following:

start Beeline with "bin/beeline",

execute "!connect jdbc:hive2://localhost:10000" or "bin/beeline -u jdbc:hive2://$HS2_HOST:$HS2_PORT" if connecting remotely,

execute "!tables" and "!dbinfo".


### Hive troubleshooting

* Problem 1: Hortonworks Hive Out of Memory/VERTEX_FAILURE

Example: https://community.rapidminer.com/t5/RapidMiner-Radoop-Forum/Performance-Node-Out-of-Memory-Error/m-p/35303#M72
Possible solution: https://community.hortonworks.com/articles/14309/demystify-tez-tuning-step-by-step.html
Possible solution: https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.4/bk_installing_manually_book/content/determine-hdp-memory-config.html

